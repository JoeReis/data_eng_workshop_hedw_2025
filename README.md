# ğŸ› ï¸ Data Engineering Lab: Building a Modern Data and AI Pipeline, HEDW 2025

Welcome to the lab! The goal of this lab is to explore parts of the data enginering lifecycle using open-source tools. You'll learn traditional data pipeline and transformation and dive into using AI to query your data warehouse.

You will:

- Create tables and data in a Postgres database.
- Extract data from the Postgres source database.
- Ingest data efficiently using Data Load Tool (dlt).
- Store and manage data within DuckDB.
- Develop a Kimball-style dimensional model using SQL.
- Serve the modeled data interactively via a Streamlit application.
- Experiment with AI querying capabilities using Ollama, Llama, and LangChain, presented through another Streamlit app.

This lab is part of a broader full-day data engineering workshop.

ğŸ¯ Learning Goals

By the end of this lab, you will:
- Understand how to use dlt to extract and load data into DuckDB
- Create a simple data warehouse for analytics and reporting
- Use Streamlit to build a simple data app
- Use AI tools LangChain and Ollama to generate SQL from natural language

ğŸ§° Tools & Technologies
- PostgreSQL â€“ Source system
- DLT (Data Load Tool) â€“ Ingestion framework
- DuckDB â€“ Analytical storage engine
- SQL â€“ Data transformation and modeling
- Python â€“ Pipeline and app development
- Streamlit â€“ Interactive data application
- Ollama + LangChain â€“ Text-to-SQL interface via LLMs

ğŸ–¥ï¸ Environment Setup

This workshop runs entirely in GitHub Codespaces. No local setup is required.
1.	Open the lab repository in your browser.
2.	Click â€œCodeâ€ > â€œCodespacesâ€ > â€œCreate codespace on mainâ€.
3.	Wait for the Codespace to initialize. This may take a minute or two.



