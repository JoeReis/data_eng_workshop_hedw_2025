# 🛠️ Data Engineering Lab: Building a Modern Data and AI Pipeline, HEDW 2025

Welcome to the lab! The goal of this lab is to explore parts of the data enginering lifecycle using open-source tools. You'll learn traditional data pipeline and transformation and dive into using AI to query your data warehouse.

You will:

- Create tables and data in a Postgres database.
- Extract data from the Postgres source database.
- Ingest data efficiently using Data Load Tool (dlt).
- Store and manage data within DuckDB.
- Develop a Kimball-style dimensional model using SQL.
- Serve the modeled data interactively via a Streamlit application.
- Experiment with AI querying capabilities using Ollama, Llama, and LangChain, presented through another Streamlit app.

This lab is part of a broader full-day data engineering workshop.

🎯 Learning Goals

By the end of this lab, you will:
- Understand how to use dlt to extract and load data into DuckDB
- Create a simple data warehouse for analytics and reporting
- Use Streamlit to build a simple data app
- Use AI tools LangChain and Ollama to generate SQL from natural language

🧰 Tools & Technologies
- PostgreSQL – Source system
- DLT (Data Load Tool) – Ingestion framework
- DuckDB – Analytical storage engine
- SQL – Data transformation and modeling
- Python – Pipeline and app development
- Streamlit – Interactive data application
- Ollama + LangChain – Text-to-SQL interface via LLMs

🖥️ Environment Setup

This workshop runs entirely in GitHub Codespaces. No local setup is required.
1.	Open the lab repository in your browser.
2.	Click “Code” > “Codespaces” > “Create codespace on main”.
3.	Wait for the Codespace to initialize. This may take a minute or two.



